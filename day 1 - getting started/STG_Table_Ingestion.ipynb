{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3adf04e-f4b6-4303-aede-6145f3101747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################### STG automation ####################\n",
    "\n",
    "## To be deleted\n",
    "# Need to dynamise it. This is a PoC. \n",
    "# Wow.. It's working (party) (party)\n",
    "# One notebook to rule them all.. all the way down\n",
    "\n",
    "path = '/Volumes/odp-aws-dls3-eu-central-1-a-sources/slsi_adb/raw_databricks_stg/IN/SAP-P3B_100_FL4137-ZCOL-ZCIC-Complaints_FTP_FULL_ALL_20241010-003037_RAW_20241010-062508.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "897d28ac-3c15-4926-9b40-5f4fe383b40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_date\n",
    "from pyspark.sql.types import StringType\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DynamicColumnMapping\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "968bf216-989e-4920-8406-8187a0fe8da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is a Sample for Config file\n",
    "\n",
    "# [\n",
    "#   {\n",
    "#     \"table_name\": \"table1\",\n",
    "#     \"csv_path\": \"/mnt/volume/data_table1.csv\",\n",
    "#     \"mapping_path\": \"/mnt/volume/mapping_table1.json\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"table_name\": \"table2\",\n",
    "#     \"csv_path\": \"/mnt/volume/data_table2.csv\",\n",
    "#     \"mapping_path\": \"/mnt/volume/mapping_table2.json\"\n",
    "#   }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca11d484-367e-4c89-8ad0-a1eef96f3269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Column Mapping file \n",
    "# [\n",
    "#   {\n",
    "#     \"csv_column\": \"csv_col1\",\n",
    "#     \"table_column\": \"table_col1\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"csv_column\": \"csv_col2\",\n",
    "#     \"table_column\": \"table_col2\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"csv_column\": \"csv_col3\",\n",
    "#     \"table_column\": \"table_col3\"\n",
    "#   }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e1e5ec3-0ba7-473e-b140-d57d331c4857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load configuration file\n",
    "config_file_path = \"/mnt/volume/config.json\"  # Path to the config file\n",
    "with open(config_file_path, \"r\") as file:\n",
    "    config_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdf12b97-915f-4d22-a027-92ae216b12ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table_config in config_data:\n",
    "    table_name = table_config[\"table_name\"]\n",
    "    csv_path = table_config[\"csv_path\"]\n",
    "    mapping_path = table_config[\"mapping_path\"]\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing table: {table_name}\")\n",
    "\n",
    "        # Load column mapping from JSON file\n",
    "        mapping_df = spark.read.json(mapping_path)\n",
    "        column_mapping = {row[\"csv_column\"]: row[\"table_column\"] for row in mapping_df.collect()}\n",
    "\n",
    "        ## To fetch the latest file\n",
    "        files = dbutils.fs.ls(csv_path)\n",
    " \n",
    "        # Sort the files by modification time in descending order (most recent first)\n",
    "        latest_file = sorted(files, key=lambda f: f.modificationTime, reverse=True)[0]\n",
    "        csv_file_path = latest_file.path\n",
    "\n",
    "        print(csv_file_path)\n",
    "\n",
    "        # Load CSV data to be transformed\n",
    "        csv_df = spark.read.option(\"header\", True).csv(csv_file_path)\n",
    "\n",
    "        # Dynamically rename columns based on the mapping\n",
    "        renamed_df = csv_df.select([col(c).alias(column_mapping[c]) if c in column_mapping else col(c) for c in csv_df.columns])\n",
    "\n",
    "        # Convert all column data types to StringType\n",
    "        string_df = renamed_df.select([col(c).cast(StringType()).alias(c) for c in renamed_df.columns])\n",
    "\n",
    "        # Add export_date column with the current date\n",
    "        final_df = string_df.withColumn(\"export_date\", current_date().cast(StringType()))\n",
    "\n",
    "        # Show transformed DataFrame\n",
    "        final_df.show()\n",
    "\n",
    "        # Truncate the table before loading data\n",
    "        spark.sql(f\"TRUNCATE TABLE {table_name}\")\n",
    "\n",
    "        # Write the DataFrame to the table\n",
    "        final_df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "        print(f\"Completed processing for table: {table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error and continue with the next table\n",
    "        print(f\"Error processing table: {table_name}\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        # continue  # Proceed to the next table"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "STG_Table_Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
